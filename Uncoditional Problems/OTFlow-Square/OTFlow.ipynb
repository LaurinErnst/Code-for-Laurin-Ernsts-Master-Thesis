{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch import distributions\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi.py\n",
    "# neural network to model the potential function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import math\n",
    "\n",
    "def antiderivTanh(x): # activation function aka the antiderivative of tanh\n",
    "    return torch.abs(x) + torch.log(1+torch.exp(-2.0*torch.abs(x)))\n",
    "\n",
    "def derivTanh(x): # act'' aka the second derivative of the activation function antiderivTanh\n",
    "    return 1 - torch.pow( torch.tanh(x) , 2 )\n",
    "\n",
    "class ResNN(nn.Module):\n",
    "    def __init__(self, d, m, nTh=2):\n",
    "        \"\"\"\n",
    "            ResNet N portion of Phi\n",
    "\n",
    "            This implementation was first described in:\n",
    "\n",
    "            @article{onken2020otflow,\n",
    "               title={OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport},\n",
    "                author={Derek Onken and Samy Wu Fung and Xingjian Li and Lars Ruthotto},\n",
    "                year={2020},\n",
    "                journal = {arXiv preprint arXiv:2006.00104},\n",
    "            }\n",
    "        :param d:   int, dimension of space input (expect inputs to be d+1 for space-time)\n",
    "        :param m:   int, hidden dimension\n",
    "        :param nTh: int, number of resNet layers , (number of theta layers)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if nTh < 2:\n",
    "            print(\"nTh must be an integer >= 2\")\n",
    "            exit(1)\n",
    "\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        self.nTh = nTh\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.append(nn.Linear(d + 1, m, bias=True)) # opening layer\n",
    "        self.layers.append(nn.Linear(m,m, bias=True)) # resnet layers\n",
    "        for i in range(nTh-2):\n",
    "            self.layers.append(copy.deepcopy(self.layers[1]))\n",
    "        self.act = antiderivTanh\n",
    "        self.h = 1.0 / (self.nTh-1) # step size for the ResNet\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            N(s;theta). the forward propogation of the ResNet\n",
    "        :param x: tensor nex-by-d+1, inputs\n",
    "        :return:  tensor nex-by-m,   outputs\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.act(self.layers[0].forward(x))\n",
    "\n",
    "        for i in range(1,self.nTh):\n",
    "            x = x + self.h * self.act(self.layers[i](x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    def __init__(self, nTh, m, d, r=10, alph=[1.0] * 5):\n",
    "        \"\"\"\n",
    "            neural network approximating Phi (see Eq. (9) in our paper)\n",
    "\n",
    "            Phi( x,t ) = w'*ResNet( [x;t]) + 0.5*[x' t] * A'A * [x;t] + b'*[x;t] + c\n",
    "\n",
    "        :param nTh:  int, number of resNet layers , (number of theta layers)\n",
    "        :param m:    int, hidden dimension\n",
    "        :param d:    int, dimension of space input (expect inputs to be d+1 for space-time)\n",
    "        :param r:    int, rank r for the A matrix\n",
    "        :param alph: list, alpha values / weighted multipliers for the optimization problem\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.m    = m\n",
    "        self.nTh  = nTh\n",
    "        self.d    = d\n",
    "        self.alph = alph\n",
    "\n",
    "        r = min(r,d+1) # if number of dimensions is smaller than default r, use that\n",
    "\n",
    "        self.A  = nn.Parameter(torch.zeros(r, d+1) , requires_grad=True)\n",
    "        self.A  = nn.init.xavier_uniform_(self.A)\n",
    "        self.c  = nn.Linear( d+1  , 1  , bias=True)  # b'*[x;t] + c\n",
    "        self.w  = nn.Linear( m    , 1  , bias=False)\n",
    "\n",
    "        self.N = ResNN(d, m, nTh=nTh)\n",
    "\n",
    "        # set initial values\n",
    "        self.w.weight.data = torch.ones(self.w.weight.data.shape)\n",
    "        self.c.weight.data = torch.zeros(self.c.weight.data.shape)\n",
    "        self.c.bias.data   = torch.zeros(self.c.bias.data.shape)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" calculating Phi(s, theta)...not used in OT-Flow \"\"\"\n",
    "\n",
    "        # force A to be symmetric\n",
    "        symA = torch.matmul(torch.t(self.A), self.A) # A'A\n",
    "\n",
    "        return self.w( self.N(x)) + 0.5 * torch.sum( torch.matmul(x , symA) * x , dim=1, keepdims=True) + self.c(x)\n",
    "\n",
    "\n",
    "    def trHess(self,x,d=None, justGrad=False ):\n",
    "        \"\"\"\n",
    "        compute gradient of Phi wrt x and trace(Hessian of Phi); see Eq. (11) and Eq. (13), respectively\n",
    "        recomputes the forward propogation portions of Phi\n",
    "\n",
    "        :param x: input data, torch Tensor nex-by-d\n",
    "        :param justGrad: boolean, if True only return gradient, if False return (grad, trHess)\n",
    "        :return: gradient , trace(hessian)    OR    just gradient\n",
    "        \"\"\"\n",
    "\n",
    "        # code in E = eye(d+1,d) as index slicing instead of matrix multiplication\n",
    "        # assumes specific N.act as the antiderivative of tanh\n",
    "\n",
    "        N    = self.N\n",
    "        m    = N.layers[0].weight.shape[0]\n",
    "        nex  = x.shape[0] # number of examples in the batch\n",
    "        if d is None:\n",
    "            d    = x.shape[1]-1\n",
    "        symA = torch.matmul(self.A.t(), self.A)\n",
    "\n",
    "        u = [] # hold the u_0,u_1,...,u_M for the forward pass\n",
    "        z = N.nTh*[None] # hold the z_0,z_1,...,z_M for the backward pass\n",
    "        # preallocate z because we will store in the backward pass and we want the indices to match the paper\n",
    "\n",
    "        # Forward of ResNet N and fill u\n",
    "        opening     = N.layers[0].forward(x) # K_0 * S + b_0\n",
    "        u.append(N.act(opening)) # u0\n",
    "        feat = u[0]\n",
    "\n",
    "        for i in range(1,N.nTh):\n",
    "            feat = feat + N.h * N.act(N.layers[i](feat))\n",
    "            u.append(feat)\n",
    "\n",
    "        # going to be used more than once\n",
    "        tanhopen = torch.tanh(opening) # act'( K_0 * S + b_0 )\n",
    "\n",
    "        # compute gradient and fill z\n",
    "        for i in range(N.nTh-1,0,-1): # work backwards, placing z_i in appropriate spot\n",
    "            if i == N.nTh-1:\n",
    "                term = self.w.weight.t()\n",
    "            else:\n",
    "                term = z[i+1]\n",
    "\n",
    "            # z_i = z_{i+1} + h K_i' diag(...) z_{i+1}\n",
    "            z[i] = term + N.h * torch.mm( N.layers[i].weight.t() , torch.tanh( N.layers[i].forward(u[i-1]) ).t() * term)\n",
    "\n",
    "        # z_0 = K_0' diag(...) z_1\n",
    "        z[0] = torch.mm( N.layers[0].weight.t() , tanhopen.t() * z[1] )\n",
    "        grad = z[0] + torch.mm(symA, x.t() ) + self.c.weight.t()\n",
    "\n",
    "        if justGrad:\n",
    "            return grad.t()\n",
    "\n",
    "        # -----------------\n",
    "        # trace of Hessian\n",
    "        #-----------------\n",
    "\n",
    "        # t_0, the trace of the opening layer\n",
    "        Kopen = N.layers[0].weight[:,0:d]    # indexed version of Kopen = torch.mm( N.layers[0].weight, E  )\n",
    "        temp  = derivTanh(opening.t()) * z[1]\n",
    "        trH  = torch.sum(temp.reshape(m, -1, nex) * torch.pow(Kopen.unsqueeze(2), 2), dim=(0, 1)) # trH = t_0\n",
    "\n",
    "        # grad_s u_0 ^ T\n",
    "        temp = tanhopen.t()   # act'( K_0 * S + b_0 )\n",
    "        Jac  = Kopen.unsqueeze(2) * temp.unsqueeze(1) # K_0' * act'( K_0 * S + b_0 )\n",
    "        # Jac is shape m by d by nex\n",
    "\n",
    "        # t_i, trace of the resNet layers\n",
    "        # KJ is the K_i^T * grad_s u_{i-1}^T\n",
    "        for i in range(1,N.nTh):\n",
    "            KJ  = torch.mm(N.layers[i].weight , Jac.reshape(m,-1) )\n",
    "            KJ  = KJ.reshape(m,-1,nex)\n",
    "            if i == N.nTh-1:\n",
    "                term = self.w.weight.t()\n",
    "            else:\n",
    "                term = z[i+1]\n",
    "\n",
    "            temp = N.layers[i].forward(u[i-1]).t() # (K_i * u_{i-1} + b_i)\n",
    "            t_i = torch.sum(  ( derivTanh(temp) * term ).reshape(m,-1,nex)  *  torch.pow(KJ,2) ,  dim=(0, 1) )\n",
    "            trH  = trH + N.h * t_i  # add t_i to the accumulate trace\n",
    "            if i < N.nTh:\n",
    "                Jac = Jac + N.h * torch.tanh(temp).reshape(m, -1, nex) * KJ # update Jacobian\n",
    "\n",
    "        return grad.t(), trH + torch.trace(symA[0:d,0:d])\n",
    "        # indexed version of: return grad.t() ,  trH + torch.trace( torch.mm( E.t() , torch.mm(  symA , E) ) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import pad\n",
    "from torch import distributions\n",
    "\n",
    "\n",
    "class OTFlow(nn.Module):\n",
    "    \"\"\"\n",
    "    OT-Flow for density estimation and sampling as described in\n",
    "\n",
    "    @article{onken2020otflow,\n",
    "        title={OT-Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport},\n",
    "        author={Derek Onken and Samy Wu Fung and Xingjian Li and Lars Ruthotto},\n",
    "        year={2020},\n",
    "        journal = {arXiv preprint arXiv:2006.00104},\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, net, nt, alph, prior, T=1.0):\n",
    "        \"\"\"\n",
    "        Initialize OT-Flow\n",
    "\n",
    "        :param net: network for value function\n",
    "        :param nt: number of rk4 steps\n",
    "        :param alph: penalty parameters\n",
    "        :param prior: latent distribution, e.g., distributions.MultivariateNormal(torch.zeros(d), torch.eye(d))\n",
    "        \"\"\"\n",
    "        super(OTFlow, self).__init__()\n",
    "        self.prior = prior\n",
    "        self.nt = nt\n",
    "        self.T = T\n",
    "        self.net = net\n",
    "        self.alph = alph\n",
    "\n",
    "\n",
    "    def g(self, z, nt = None, storeAll=False):\n",
    "        \"\"\"\n",
    "        :param z: latent variable\n",
    "        :return: g(z) and hidden states\n",
    "        \"\"\"\n",
    "        return self.integrate(z,[self.T, 0.0], nt,storeAll)\n",
    "\n",
    "    def ginv(self, x, nt=None, storeAll=False):\n",
    "        \"\"\"\n",
    "        :param x: sample from dataset\n",
    "        :return: g^(-1)(x), value of log-determinant, and hidden layers\n",
    "        \"\"\"\n",
    "\n",
    "        return self.integrate(x,[0.0, self.T], nt,storeAll)\n",
    "\n",
    "    def log_prob(self, x, nt=None):\n",
    "        \"\"\"\n",
    "        Compute log-probability of a sample using change of variable formula\n",
    "\n",
    "        :param x: sample from dataset\n",
    "        :return: logp_{\\theta}(x)\n",
    "        \"\"\"\n",
    "        z, _, log_det_ginv, v, r = self.ginv(x,nt)\n",
    "        return self.prior.log_prob(z) - log_det_ginv, v, r\n",
    "\n",
    "    def sample(self, s,nt=None):\n",
    "        \"\"\"\n",
    "        Draw random samples from p_{\\theta}\n",
    "\n",
    "        :param s: number of samples to draw\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z = self.prior.sample((s, 1)).squeeze(1)\n",
    "        x, _, _, _, _ = self.g(z,nt)\n",
    "        return x\n",
    "\n",
    "    def f(self,x, t):\n",
    "        \"\"\"\n",
    "        neural ODE combining the characteristics and log-determinant (see Eq. (2)), the transport costs (see Eq. (5)), and\n",
    "        the HJB regularizer (see Eq. (7)).\n",
    "\n",
    "        d_t  [x ; l ; v ; r] = odefun( [x ; l ; v ; r] , t )\n",
    "\n",
    "        x - particle position\n",
    "        l - log determinant\n",
    "        v - accumulated transport costs (Lagrangian)\n",
    "        r - accumulates violation of HJB condition along trajectory\n",
    "        \"\"\"\n",
    "        nex, d = x.shape\n",
    "        z = pad(x[:, :d], (0, 1, 0, 0), value=t)\n",
    "        gradPhi, trH = self.net.trHess(z)\n",
    "\n",
    "        dx = -(1.0 / self.alph[0]) * gradPhi[:, 0:d]\n",
    "        dl = (1.0 / self.alph[0]) * trH\n",
    "        # dv = 0.5 * torch.sum(torch.pow(dx, 2), 1)\n",
    "        # dr = torch.abs(-gradPhi[:, -1] + self.alph[0] * dv)\n",
    "\n",
    "        return dx, dl, dv, dr\n",
    "\n",
    "    def integrate(self, y, tspan, nt=None,storeAll=False):\n",
    "        \"\"\"\n",
    "        RK4 time-stepping to integrate the neural ODE\n",
    "\n",
    "        :param y: initial state\n",
    "        :param tspan: time interval (can go backward in time)\n",
    "        :param nt: number of time steps (default is self.nt)\n",
    "        :return: y (final state), ys (all states), l (log determinant), v (transport costs), r (HJB penalty)\n",
    "        \"\"\"\n",
    "        if nt is None:\n",
    "            nt = self.nt\n",
    "\n",
    "        nex, d = y.shape\n",
    "        h = (tspan[1] - tspan[0])/ nt\n",
    "        tk = tspan[0]\n",
    "\n",
    "        l = torch.zeros((nex), device=y.device, dtype=y.dtype)\n",
    "        v = torch.zeros((nex), device=y.device, dtype=y.dtype)\n",
    "        r = torch.zeros((nex), device=y.device, dtype=y.dtype)\n",
    "        if storeAll:\n",
    "            ys = [torch.clone(y).detach().cpu()]\n",
    "        else:\n",
    "            ys = None\n",
    "\n",
    "        w =  [(h/6.0),2.0*(h/6.0),2.0*(h/6.0),1.0*(h/6.0)]\n",
    "        for i in range(nt):\n",
    "            y0 = y\n",
    "\n",
    "            dy, dl, dv, dr = self.f(y0, tk)\n",
    "            y = y0 + w[0] * dy\n",
    "            l += w[0] * dl\n",
    "            v += w[0] * dv\n",
    "            r += w[0] * dr\n",
    "\n",
    "            dy, dl, dv, dr =  self.f(y0 + 0.5 * h * dy, tk + (h / 2))\n",
    "            y += w[1] * dy\n",
    "            l += w[1] * dl\n",
    "            v += w[1] * dv\n",
    "            r += w[1] * dr\n",
    "\n",
    "            dy, dl, dv, dr = self.f(y0 + 0.5 * h * dy, tk + (h / 2))\n",
    "            y += w[2] * dy\n",
    "            l += w[2] * dl\n",
    "            v += w[2] * dv\n",
    "            r += w[2] * dr\n",
    "\n",
    "            dy, dl, dv, dr = self.f(y0 + h * dy, tk + h)\n",
    "            y += w[3] * dy\n",
    "            l += w[3] * dl\n",
    "            v += w[3] * dv\n",
    "            r += w[3] * dr\n",
    "\n",
    "            if storeAll:\n",
    "                ys.append(torch.clone(y).detach().cpu())\n",
    "            tk +=h\n",
    "\n",
    "        return y, ys, l, v, r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = []\n",
    "distance = 0.4\n",
    "sample_multiplier = 10\n",
    "# 16\n",
    "# for i in np.arange(0, 8, distance):\n",
    "#     for j in np.arange(0, 2, distance):\n",
    "#         x.append([j, i])\n",
    "for i in range(16 * sample_multiplier):\n",
    "    x.append([np.random.uniform(2, 6), np.random.uniform(2, 6)])\n",
    "\n",
    "\n",
    "\n",
    "x = torch.tensor(x)\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD(x, y):\n",
    "        gamma = 2\n",
    "        xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "        rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "        ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "\n",
    "        dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "        dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "        rxx = rx[0].repeat(y.shape[0], 1)\n",
    "        ryy = ry[0].repeat(x.shape[0], 1) \n",
    "        dxy = rxx.t() + ryy - 2. * zz # Used for C in (1)\n",
    "\n",
    "        XX, YY, XY = (torch.zeros(xx.shape).to(device),\n",
    "                      torch.zeros(yy.shape).to(device),\n",
    "                      torch.zeros(zz.shape).to(device))\n",
    "        XX += 1/(1 + dxx/gamma**2)\n",
    "        YY += 1/(1 + dyy/gamma**2)\n",
    "        XY += 1/(1 + dxy/gamma**2)\n",
    "        return XX.mean() + YY.mean() - 2*XY.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 999, Loss: 47.059349060058594\n",
      "Step: 1999, Loss: 45.915950775146484\n",
      "Step: 2999, Loss: 44.71723175048828\n",
      "Step: 3999, Loss: 44.58055877685547\n",
      "Step: 4999, Loss: 44.39386749267578\n",
      "Step: 999, Loss: 45.93810272216797\n"
     ]
    }
   ],
   "source": [
    "f = open(\"results.txt\", \"w\")\n",
    "for i in [8, 16, 24, 32]:\n",
    "    for j in [2, 4]:\n",
    "        f.write(\"OTFlow layers: \" + str(j) + \" width: \" + str(i) + \"\\n\")\n",
    "        nTh = j # number of layers\n",
    "        width = i # width of network\n",
    "        alph = [1.0,10.0,5.0] # alph[0]-> weight for transport costs, alph[1] and alph[2]-> HJB penalties\n",
    "        net = Phi(nTh=nTh, m=width, d=2, alph=alph).to(device)\n",
    "        prior = distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))\n",
    "        nt = 2                 # number of rk4 steps to solve neural ODE\n",
    "        flow = OTFlow(net, nt, alph, prior, T=1.0)\n",
    "        loss_arr = []\n",
    "\n",
    "        batch_size = 4000\n",
    "        num_steps = 5000\n",
    "        f.write(\"MMD before training: \" + str(MMD(flow.sample(x.shape[0]), x).item()) + \"\\n\")\n",
    "\n",
    "        optim = torch.optim.Adam(net.parameters(), lr=0.01) # lr=0.04 good\n",
    "\n",
    "        dataloader = torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True)\n",
    "        sheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=2500, gamma=0.1)\n",
    "        for step in range(num_steps):\n",
    "            for x_batch in dataloader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                logp, L, P = flow.log_prob(x_batch)\n",
    "                loss = (-alph[1]*logp + alph[0] * L + alph[2] * P).mean()\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "            loss_arr.append(loss.item())\n",
    "            sheduler.step()\n",
    "            if step % 1000 == 999:\n",
    "                print(\"Step: {}, Loss: {}\".format(step, loss.item()))\n",
    "\n",
    "        f.write(\"MMD after training: \" + str(MMD(flow.sample(x.shape[0]), x).item()) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        np.savetxt(\"l\"+str(j) + \"w\" + str(i) + \".txt\", loss_arr, fmt='%f')\n",
    "\n",
    "        col_red = '#c61826'\n",
    "        col_dark_red = '#590d08'\n",
    "        col_blue = '#01024d'\n",
    "        xs = flow.sample(1000).detach().cpu()\n",
    "        plt.figure()\n",
    "        plt.scatter(xs[:, 0], xs[:, 1], s=25, alpha=1, label=\"Generated Data\", c=col_red)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.ylim(0, 8)\n",
    "        plt.xlim(0, 8)\n",
    "        plt.savefig(\"OTFlow_plot_square_gen_l\" + str(j) + \"w\" + str(i) + \".png\")\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(x[:, 0], x[:, 1], alpha = 1, s=25, label=\"Original Data\", c = col_blue)\n",
    "        plt.scatter(xs[:, 0], xs[:, 1], s=25, alpha=1, label=\"Generated Data\", c=col_red)\n",
    "        import matplotlib.patches as patches\n",
    "        square = patches.Rectangle((2, 2), 4, 4, linewidth=1, edgecolor=col_dark_red, facecolor='none')\n",
    "\n",
    "        plt.gca().add_patch(square)\n",
    "        plt.ylim(0, 8)\n",
    "        plt.xlim(0, 8)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.legend()\n",
    "        plt.savefig(\"OTFlow_plot_square_gen_smp_l\" + str(j) + \"w\" + str(i) + \".png\")\n",
    "        plt.legend()\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        plt.semilogy(loss_arr)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Log Log Likelihood Loss\")\n",
    "        plt.ylim(50, 200)\n",
    "        plt.savefig(\"OTFlow_square_semilog_loss_plot_l\" + str(j) + \"w\" + str(i) + \".png\")\n",
    "        plt.close()\n",
    "        plt.figure()\n",
    "        plt.plot(loss_arr)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Log Likelihood Loss\")\n",
    "        plt.ylim(25, 100)\n",
    "        plt.savefig(\"OTFlow_square_loss_plot_l\" + str(j) + \"w\" + str(i) + \".png\")\n",
    "        plt.close()\n",
    "\n",
    "f.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
